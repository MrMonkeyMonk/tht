[{"uri":"https://ethack.github.io/tht/","title":"Introduction","tags":[],"description":"","content":"Threat Hunting Toolkit You have data. Now what? Don\u0026rsquo;t panic! You have many tools and methods at your disposal. This documentation will show how you can use and combine them to make your data more valuable.\n Visualize  The simplest thing you can do is look at your raw data. There may be too much at first to be meaningful, but viewing your data is necessary to determine how to transform and summarize it.  Example: Using cat to view a log file or loading a CSV into a spreadsheet.   Creating graphs and charts is a useful way to discover patterns and trends, identify outliers, and view relationships.   Clean  An overlooked, but crucial step is making sure your data is cleaned or normalized so that other operations can be performed without error. This is an important part of data science.   Filter  Reducing the amount of data through filtering is nearly always going to be your first step and often between other operations as well. Not only does this remove noise irrelevant to the question or goal you have, but it reduces the amount of time it takes to get to that goal.   Sort  Sorting the data on different or even multiple fields is necessary for techniques like long tail analysis. It is also a vital first step done before any aggregation/grouping procedure.   Compute  This is when you use fields from your existing data and perform some operation to create or derive a new field, sometimes referred to as a computed field. A simple example would be adding the bytes sent and received together to get the total bytes transferred.   Correlate  You can correlate your data with other data by joining datasets together on common fields.  Example: Taking multiple log types that share a field (e.g. IP address) and joining them together to create a single entry containing data from both logs.   You can also enrich your data using data from other sources, such as an API or existing database.  Examples: Name resolution or passive DNS, geographical info, WHOIS or ownership info, or even threat intelligence feeds.     Summarize  Summarizing, also known as grouping, aggregating, or stacking, reduces your data by combining data through methods such as the average, sum, or count.      "},{"uri":"https://ethack.github.io/tht/filter/","title":"Filter","tags":[],"description":"","content":"Chapter X Some Chapter title Lorem Ipsum.\n"},{"uri":"https://ethack.github.io/tht/summarize/","title":"Summarize","tags":[],"description":"","content":"Chapter X Some Chapter title Lorem Ipsum.\n"},{"uri":"https://ethack.github.io/tht/correlate/","title":"Correlate","tags":[],"description":"","content":"Chapter X Some Chapter title Lorem Ipsum.\n"},{"uri":"https://ethack.github.io/tht/visualize/","title":"Visualize","tags":[],"description":"","content":"Chapter X Some Chapter title Lorem Ipsum.\n"},{"uri":"https://ethack.github.io/tht/multipurpose/","title":"Multipurpose","tags":[],"description":"","content":"Chapter X Some Chapter title Lorem Ipsum.\n"},{"uri":"https://ethack.github.io/tht/utils/","title":"Utils","tags":[],"description":"","content":"Chapter X Some Chapter title Lorem Ipsum.\n"},{"uri":"https://ethack.github.io/tht/development/","title":"Development","tags":[],"description":"","content":"Running tht --dev This will mount certain files from this repository directly into the container. This allows making changes to scripts without having to rebuild the image.\nBuilding Docker Image You can build the image manually with the following command. However, the Maxmind geo information will not be included.\ndocker build -t ethack/tht . If you want to build with Maxmind data, you\u0026rsquo;ll need to sign up for a free Maxmind license key. You can then specify your key when building.\ndocker build --build-arg MAXMIND_LICENSE=yourkeyhere -t ethack/tht . Building Documentation You\u0026rsquo;ll need Hugo. Official install instructions are here though the easiest way is to grab the latest Github release for your platform. Hugo is a single binary so there\u0026rsquo;s really not much to install.\nOnce you have Hugo you can compile and host the documentation locally with:\nhugo -s docs server -D Then access the local website http://localhost:1313 in your browser.\n Note:If you see a blank page or see warnings from hugo like found no layout file you need to download the theme, which you can do with: git submodule update --init\n "},{"uri":"https://ethack.github.io/tht/filter/filter/","title":"Filter","tags":[],"description":"","content":"Overview The filter script is tailored for searching IP addresses and domains in Zeek logs. However, it is flexible enough that it can be used for other purpsoses as well.\nfilter has several special features. By default, it:\n Searches in the current directory tree for any conn.log files, including compressed files and rotated logs. Searches files in parallel using multiple cores. Uses faster alternatives to grep like ripgrep or ugrep, when available. Keeps Zeek TSV headers intact so that tools like zeek-cut can be used on the results. Requires all search terms to be found in the same line to cut down on piping to repeated searches. Escapes periods in search terms to prevent 10.0.1.0 from matching 10.0.100 or google.com from matching google1com.com. Adds boundaries around the search term to prevent 192.168.1 from matching 192.168.100.50 or google.com from matching fakegoogle.com.  Usage filter [--\u0026lt;logtype\u0026gt;] [OPTIONS] [search_term] [search_term...] [-- [OPTIONS]] --\u0026lt;logtype\u0026gt; is used to search logs of \u0026#34;logtype\u0026#34; (e.g. conn, dns, etc) in the current directory tree (default: conn) Specify one or more [search_terms] to filter either STDIN or log files. If you don\u0026#39;t specify any search terms, all lines will be printed. Lines must match all search terms by default. -o|--or at least one search term is required to appear in a line (as opposed to all terms matching) Search terms will match on word boundaries by default. -s|--starts-with anchor search term to beginning of field (e.g. 192.168) -e|--ends-with anchor search term to end of field (e.g. google.com) -r|--regex signifies that [search_term(s)] should be treated as regexes -n|--dry-run print out the final search command rather than execute it filter will find the first search tool available. Use the following options to force a specific tool. --rg force use of ripgrep --ug force use of ugrep --zgrep force use of zgrep Any arguments given after -- will be passed to the underlying search command. Comparison with Grep The best way to understand the benefits is with an example. filter is designed to be called as-is for the most common use cases. It would defeat its purpose if it required extra options every time.\n# find all conn log entries from the current directory tree containing 192.168.1.1 filter 192.168.1.1 Let\u0026rsquo;s look at what it would take to do this without using filter. A first attempt is straightforward:\ncat conn.*log | fgrep 192.168.1.1 # or zcat conn.*log.gz | fgrep 192.168.1.1 However, this has multiple issues:\n It is cumbersome to search both plaintext and gzip logs at once. Zeek TSV headers are stripped which means no piping to zeek-cut. grep doesn\u0026rsquo;t make use of multiple cores, increasing the search time. Logs in subdirectories are not searched. Search will match 192.168.1.10, 192.168.1.19, 192.168.1.100, etc.  There are ways around each of these issues, but they add extra complexity to the command and more typing. You\u0026rsquo;d ultimately end up with something like this:\nfind . -regextype egrep -iregex \u0026#39;.*/conn\\b.*\\.log(.gz)?$\u0026#39; | xargs -n 1 -P $(nproc) zgrep -e \u0026#39;^#\u0026#39; -e \u0026#39;\\b192\\.168\\.1\\.1\\b\u0026#39; Which is roughly equivalent to the much shorter filter 192.168.1.1.\nExamples Let\u0026rsquo;s take a look at more examples.\nJust like traditional grep you can also pipe text to search as input.\n # log entries from stdin containing 192.168.1.1 cat conn.log | filter 192.168.1.1 # conn log entries containing 192.168; note: this could also match 10.10.192.168 filter 192.168 # conn log entries containing both 192.168.1.1 and 8.8.8.8 filter 192.168.1.1 8.8.8.8 # dns log entries containing 8.8.8.8 filter --dns 8.8.8.8 # dns log entries to Google or Cloudflare\u0026#39;s DNS servers filter --dns --or 8.8.8.8 8.8.4.4 1.1.1.1 1.0.0.1 # http log entries containing google.com; note: this will also match google.com.fake.com filter --http google.com # conn JSON entries where the origin host is 192.168.1.1 filter --regex \u0026#39;\u0026#34;id.orig_h\u0026#34;:\u0026#34;192.168.1.1\u0026#34;\u0026#39; Where filter really shines is when you combine it with other tools that can parse Zeek logs, such as zeek-cut, conn-summary, and zq.\n# find all source IPs that queried evil.com filter --dns evil.com | zeek-cut id.orig_h | sort -V | uniq # find all IPs there were resolved by evil.com queries filter --dns evil.com | zeek-cut answers | filter -p ipv4 -- -o | sort -V | uniq # show a summary of traffic involving a specific IP address filter 1.2.3.4 | conn-summary # TODO zq example with a grouping filter --or -f patterns.txt # or using xargs; note that --conn or other file specification is necessary here cat patterns.txt | xargs filter --conn --or Performance Use filter to quickly reduce log volume before piping to more specialized tools.\n By design, filter will match the search string anywhere in the line. This means that if you want to search for an origin of 192.168.1.1, the best method is to first use filter and then another tool that can check a certain field such as awk, jq, zq.\n# TODO awk example # for JSON logs you can do something like this filter --regex \u0026#39;\u0026#34;id.orig_h\u0026#34;:\u0026#34;192.168.1.1\u0026#34;\u0026#39; # or this filter 192.168.1.1 | jq \u0026#39;select(.\u0026#34;id.orig_h\u0026#34;==\u0026#34;192.168.1.1\u0026#34;)\u0026#39; # or you can use zq for either type of Zeek log filter 192.168.1.1 | zq -f zeek \u0026#34;id.orig_h = 192.168.1.1\u0026#34; - You might be wondering in that last example why you would even need to use filter at all. You\u0026rsquo;d certainly get the same results by using the zq command on the unfiltered logs.\nIt comes down to performance. String searching tools like grep are going to be much faster than something like zq or jq that parses and interprets the field values in a log file. The benchmarks below show that it is quite a bit faster to first use filter to reduce log volume before doing more expensive matching, than it is to feed all the logs directly into zq or jq for field matching first.\nSince filter uses multiple CPU cores, this also has the benefit of parallelizing the pipeline stage when the log volume is at its peak.\n $ hyperfine -w 1 \\  -n filter-then-select \u0026#39;filter 10.55.182.100 | zq -f zeek \u0026#34;id.orig_h = 10.55.182.100\u0026#34; -\u0026#39; \\  -n cat-then-select \u0026#39;zcat conn.* | zq -f zeek \u0026#34;id.orig_h = 10.55.182.100\u0026#34; -\u0026#39; Benchmark #1: filter-then-select Time (mean ± σ): 768.8 ms ± 31.5 ms [User: 3.399 s, System: 0.262 s] Range (min … max): 714.9 ms … 822.9 ms 10 runs Benchmark #2: cat-then-select Time (mean ± σ): 6.256 s ± 0.392 s [User: 11.595 s, System: 0.466 s] Range (min … max): 5.807 s … 7.240 s 10 runs Summary \u0026#39;filter-then-select\u0026#39; ran 8.14 ± 0.61 times faster than \u0026#39;cat-then-select\u0026#39; $ hyperfine -w 1 \\  -n cat-then-select \u0026#39;cat conn.* | jq \u0026#34;select(.\\\u0026#34;id.orig_h\\\u0026#34;==\\\u0026#34;10.55.182.100\\\u0026#34;)\u0026#34;\u0026#39; \\  -n filter-then-select \u0026#39;filter 10.55.182.100 | jq \u0026#34;select(.\\\u0026#34;id.orig_h\\\u0026#34;==\\\u0026#34;10.55.182.100\\\u0026#34;)\u0026#34;\u0026#39; Benchmark #1: cat-then-select Time (mean ± σ): 22.428 s ± 2.036 s [User: 22.185 s, System: 2.264 s] Range (min … max): 18.629 s … 24.577 s 10 runs Benchmark #2: filter-then-select Time (mean ± σ): 1.285 s ± 0.101 s [User: 2.455 s, System: 0.172 s] Range (min … max): 1.096 s … 1.386 s 10 runs Summary \u0026#39;filter-then-select\u0026#39; ran 17.45 ± 2.09 times faster than \u0026#39;cat-then-select\u0026#39; Gotchas # this will also match 10.10.192.168 or 10.192.168.10, etc. filter 192.168 # do this instead filter --starts-with 192.168 # this will also match google.com.fake.com filter --http google.com # do this instead filter --http --ends-with google.com Alternatives  grepwide ripgrep ugrep grepcidr  FAQ Q: Why use filter over tools like awk, grep, jq, zq, etc.?\nA: filter complements or enhances many of these tools.\n For instance, using a regex search tool is nearly always faster than using awk, zq, or jq to perform equality testing. By assuming a specific use case (searching Zeek logs for things like IP addresses) filter can automate a bunch of boilerplate like escaping periods in regexes, passing through Zeek headers, and recursively searching compressed files of one log type. grep on its own does not utilize parallel processing. This means either replacing it with an alternative or remembering the correct syntax to combine it with something like parallel or xargs.  "},{"uri":"https://ethack.github.io/tht/summarize/conn-summary/","title":"Conn Summary","tags":[],"description":"","content":"Zeek\u0026rsquo;s conn-summary.log file contains useful information about a network as a whole.\n\u0026gt;== Total === 2018-03-11-23-58-55 - 2018-03-12-00-59-10 - Connections 20.0k - Payload 206.1m - Sampling 1.00% - Ports | Sources | Destinations | Services | Protocols | States | 53 66.0% | 10.55.200.10#1 39.0% | 172.16.200.11#2 18.0% | dns 66.0% | 17 66.0% | SF 78.5% | 443 29.5% | 10.55.200.11#3 23.5% | 165.227.88.15#4 3.0% | ssl 26.0% | 6 34.0% | S0 18.5% | 80 4.5% | 10.55.100.103#5 6.0% | 205.251.196.95#6 2.0% | http 7.5% | | RSTO 2.5% | | 10.55.100.100#7 5.0% | 172.217.4.70#8 2.0% | - 0.5% | | RSTR 0.5% | | 10.55.100.111#9 4.5% | 23.61.199.64#10 2.0% | | | | | 10.55.100.110#11 4.0% | 204.74.101.1#12 1.5% | | | | | 192.168.88.2#13 3.5% | 172.217.4.78#14 1.5% | | | | | 10.55.100.104#15 3.5% | 104.156.80.32#16 1.5% | | | | | 10.55.100.108#17 3.0% | 65.153.18.198#18 1.5% | | | | | 10.55.100.106#19 2.5% | 65.153.18.197#20 1.5% | | | | #1=\u0026lt;???\u0026gt; #2=\u0026lt;???\u0026gt; #3=\u0026lt;???\u0026gt; #4=\u0026lt;???\u0026gt; #5=\u0026lt;???\u0026gt; #6=ns-1119.awsdns-11.org #7=\u0026lt;???\u0026gt; #8=ord37s18-in-f6.1e100.net #9=\u0026lt;???\u0026gt; #10=a7-64.akam.net #11=\u0026lt;???\u0026gt; #12=udns2.ultradns.net #13=\u0026lt;???\u0026gt; #14=lga15s47-in-f78.1e100.net #15=\u0026lt;???\u0026gt; #16=\u0026lt;???\u0026gt; #17=\u0026lt;???\u0026gt; #18=\u0026lt;???\u0026gt; #19=\u0026lt;???\u0026gt; #20=\u0026lt;???\u0026gt; There\u0026rsquo;s a lot to unpack here. Each colum shows the top pieces of information, broken out by percentage of connections. For instance:\nPorts 53 66.0% 443 29.5% 80 4.5% This means that 66.0% of the connections were to port 53, 29.5% were to port 443, and the remaining 4.5% were to port 80. Note that this doesn\u0026rsquo;t specify TCP or UDP. However, the second-to-last column shows that protocol 17 (UDP) was 66.0% of the connections. This happens to match up with the percentage for port 53 so we know that UDP/53 was used.\nProtocols 17 66.0% 6 34.0% Similarly, the following columns break down the percentage of connections by both source and destination IP addresses.\nSources | Destinations 10.55.200.10#1 39.0% | 172.16.200.11#2 18.0% 10.55.200.11#3 23.5% | 165.227.88.15#4 3.0% 10.55.100.103#5 6.0% | 205.251.196.95#6 2.0% 10.55.100.100#7 5.0% | 172.217.4.70#8 2.0% 10.55.100.111#9 4.5% | 23.61.199.64#10 2.0% 10.55.100.110#11 4.0% | 204.74.101.1#12 1.5% 192.168.88.2#13 3.5% | 172.217.4.78#14 1.5% 10.55.100.104#15 3.5% | 104.156.80.32#16 1.5% 10.55.100.108#17 3.0% | 65.153.18.198#18 1.5% 10.55.100.106#19 2.5% | 65.153.18.197#20 1.5% The conn-summary.log file is generated by the trace-summary script. You can also run this script manually and access different options. The script has the ability to generate summaries from either live network data or from Zeek TSV or JSON-formatted conn.log files.\nTHT has a conn-summary script that is useful for generating summaries on the fly for many common use cases. It lets you pipe conn logs into the script. This means you can generate summaries based only on hosts or timeframes you care about.\nfilter 192.168.88.2 165.227.88.15 | conn-summary  The above example uses the filter script included with THT.\n conn-summary uses trace-summary under the hood, and the output is similar to what we saw above with the following differences:\n Internal to internal traffic is not included. That is, any connections between RFC1918 IP address ranges are excluded. Unlike the Zeek conn-summary.log the data is not sampled by default. This means the statistics presented include all the connections instead of a sample size. The percentages are output twice: once for connections and once for bytes. That means you can see both where the majority of connections went in the top table and where the majority of the data transferred went in the bottom table. The Zeek conn-summary.log includes a bunch of other breakdowns based on different IP address ranges that are not included in the output here.  Connections: ============ \u0026gt;== Total === 2018-01-30-12-14-02 - 2018-01-31-12-14-00 - Connections 239.4k - Payload 35.4m - Ports | Sources | Destinations | Services | Protocols | States | 53 98.8% | 192.168.88.2 100.0% | 165.227.88.15 90.9% | dns 98.8% | 17 100.0% | SF 99.8% | 123 1.2% | 165.227.88.15 0.0% | 208.84.2.53 0.5% | ntp 1.2% | 6 0.0% | S0 0.1% | 443 0.0% | | 208.76.45.53 0.5% | ssl 0.0% | 1 0.0% | REJ 0.0% | 3 0.0% | | 95.101.36.192 0.2% | - 0.0% | | OTH 0.0% | | | 23.211.133.192 0.1% | | | | | | 95.100.173.192 0.1% | | | | | | 95.100.168.194 0.1% | | | | | | 184.85.248.194 0.1% | | | | | | 96.7.49.194 0.1% | | | | | | 208.109.255.2 0.1% | | | | Bytes: ====== \u0026gt;== Total === 2018-01-30-12-14-02 - 2018-01-31-12-14-00 - Connections 239.4k - Payload 35.4m - Ports | Sources | Destinations | Services | Protocols | States | 53 98.3% | 192.168.88.2 100.0% | 165.227.88.15 88.2% | dns 98.3% | 17 99.1% | SF 100.0% | 443 0.9% | 165.227.88.15 0.0% | 162.208.119.40 0.9% | ssl 0.9% | 6 0.9% | OTH 0.0% | 123 0.8% | | 208.76.45.53 0.4% | ntp 0.8% | 1 0.0% | S0 0.0% | 3 0.0% | | 208.84.2.53 0.4% | - 0.0% | | REJ 0.0% | | | 95.101.36.192 0.2% | | | | | | 23.211.133.192 0.2% | | | | | | 95.100.173.192 0.2% | | | | | | 184.85.248.194 0.2% | | | | | | 184.26.161.192 0.2% | | | | | | 95.100.168.194 0.2% | | | | Alternatives  https://github.com/jbaggs/conn-summary - reads Zeek logs from Elasticsearch  "},{"uri":"https://ethack.github.io/tht/utils/hyperfine/","title":"Hyperfine","tags":[],"description":"","content":""},{"uri":"https://ethack.github.io/tht/categories/","title":"Categories","tags":[],"description":"","content":""},{"uri":"https://ethack.github.io/tht/tags/","title":"Tags","tags":[],"description":"","content":""}]